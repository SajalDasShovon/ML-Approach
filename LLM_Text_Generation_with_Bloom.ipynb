{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKetWJhg8tV1H8ERwwcmOP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajalDasShovon/ML-Approach/blob/main/LLM_Text_Generation_with_Bloom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2vXCTNIZHwZ",
        "outputId": "58ffdf20-3dab-4816-c1c3-f6f5be0a39df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.35.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, set_seed"
      ],
      "metadata": {
        "id": "BKhS1LK7hkck"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n"
      ],
      "metadata": {
        "id": "O1osrQTMhkZs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"bigscience/bloom-1b7\")\n",
        "\n",
        "#model_lm = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")"
      ],
      "metadata": {
        "id": "7kDxuqWhhkXD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ZrTLH3V-hkGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b271faf-0875-4e41-d11f-d4f85daca4e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BloomModel(\n",
              "  (word_embeddings): Embedding(250880, 2048)\n",
              "  (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  (h): ModuleList(\n",
              "    (0-23): 24 x BloomBlock(\n",
              "      (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): BloomAttention(\n",
              "        (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): BloomMLP(\n",
              "        (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (gelu_impl): BloomGelu()\n",
              "        (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__"
      ],
      "metadata": {
        "id": "nWRn1j9phkCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb88adc-7bf3-420f-9869-34d843623c08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.bloom.modeling_bloom.BloomModel"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__.__name__"
      ],
      "metadata": {
        "id": "8zjejXz1hj-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "97cacf72-8205-4185-d62e-c49bca49d530"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BloomModel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(11111)"
      ],
      "metadata": {
        "id": "sthcMYpJhj6g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt='What is life in the 20th Century?'"
      ],
      "metadata": {
        "id": "leTRuEHjjxLO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "input_tokens = tokenizer(text_prompt, return_tensors=\"pt\").to(0)"
      ],
      "metadata": {
        "id": "92hQwmIpjxJa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens"
      ],
      "metadata": {
        "id": "StzLJ3r1jxHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817e6a7e-9cba-4c41-da0e-d132030dbcca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 10560,    632,  10440,    361,    368, 106234, 103126,     34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lm = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
        "#result_sample = model.generate(**input_tokens, max_length=200, top_k=0, temperature=0.5)\n",
        ""
      ],
      "metadata": {
        "id": "KKS1N3JTjxFW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_sample = model_lm.generate(**input_tokens, max_length=200, top_k=0, temperature=0.5)"
      ],
      "metadata": {
        "id": "LJ1lf_JLjxDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16a2134-470c-4653-a252-f7f9748d9ccf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_sample"
      ],
      "metadata": {
        "id": "XKP3rplKjxBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d353fcf-abee-4c2f-c9c1-3cecff15c5db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 10560,    632,  10440,    361,    368, 106234, 103126,     34,   3162,\n",
              "            632,    267,   3509,    461,  10087,   7458,     15,    530,    368,\n",
              "           8876,    632,  35179,     17,   1387,   8876,    632,  35179,   5908,\n",
              "            461,    368,  28770,    861,    632,  35179,    368,   4676,   1701,\n",
              "          20152,     17,   1387,   8876,    632,  35179,   5908,    461,    368,\n",
              "           4676,   1701,   6482,     17,   1387,   8876,    632,  35179,   5908,\n",
              "            461,    368,   4676,   1701,  85392,     17,   1387,   8876,    632,\n",
              "          35179,   5908,    461,    368,   4676,   1701,   2909,     17,   1387,\n",
              "           8876,    632,  35179,   5908,    461,    368,   4676,   1701,  20152,\n",
              "             17,   1387,   8876,    632,  35179,   5908,    461,    368,   4676,\n",
              "           1701,   6482,     17,   1387,   8876,    632,  35179,   5908,    461,\n",
              "            368,   4676,   1701,  85392,     17,   1387,   8876,    632,  35179,\n",
              "           5908,    461,    368,   4676,   1701,   2909,     17,   1387,   8876,\n",
              "            632,  35179,   5908,    461,    368,   4676,   1701,  20152,     17,\n",
              "           1387,   8876,    632,  35179,   5908,    461,    368,   4676,   1701,\n",
              "           6482,     17,   1387,   8876,    632,  35179,   5908,    461,    368,\n",
              "           4676,   1701,  85392,     17,   1387,   8876,    632,  35179,   5908,\n",
              "            461,    368,   4676,   1701,   2909,     17,   1387,   8876,    632,\n",
              "          35179,   5908,    461,    368,   4676,   1701,  20152,     17,   1387,\n",
              "           8876,    632,  35179,   5908,    461,    368,   4676,   1701,   6482,\n",
              "             17,   1387,   8876,    632,  35179,   5908,    461,    368,   4676,\n",
              "           1701,  85392,     17,   1387,   8876,    632,  35179,   5908,    461,\n",
              "            368,   4676]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_sample[0]\n",
        "print(tokenizer.decode(result_sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
      ],
      "metadata": {
        "id": "UEGqhRvijw-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af073ca-1daf-4232-9394-3fc97d2271d1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is life in the 20th Century? It is a time of great change, and the world is changing. The world is changing because of the technology that is changing the way we live. The world is changing because of the way we think. The world is changing because of the way we communicate. The world is changing because of the way we work. The world is changing because of the way we live. The world is changing because of the way we think. The world is changing because of the way we communicate. The world is changing because of the way we work. The world is changing because of the way we live. The world is changing because of the way we think. The world is changing because of the way we communicate. The world is changing because of the way we work. The world is changing because of the way we live. The world is changing because of the way we think. The world is changing because of the way we communicate. The world is changing because of the way\n"
          ]
        }
      ]
    }
  ]
}